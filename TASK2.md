graph TD

    %% USER AUDIO INPUT
    subgraph "User Input"
        Mic["ðŸŽ¤ User Audio (5â€“10 min)"]
        Metadata["ðŸ“ User Metadata
        - User ID
        - Device info
        - Recording conditions"]
    end

    %% ENGINE PIPELINE
    subgraph "Personalization Engine"
        
        Preprocessor["ðŸ§¹ Audio Preprocessor
        - Noise reduction
        - Normalization
        - Silence removal
        - Segmentation"]

        SpeakingPattern["â±ï¸ Speaking Pattern Analyzer
        - Pacing & rhythm
        - Word/sentence pauses
        - Breathing patterns"]

        ProsodyModel["ðŸ“ˆ Stress & Pitch Model
        - F0 contours
        - Stress patterns
        - Intonation modeling"]

        EmotionDetector["ðŸ˜Š Emotion Detector
        - Happy / Sad / Neutral / Excited
        - Energy + pitch + spectral features"]

        ProfileBuilder["ðŸ§© Voice Profile Builder
        - Prosody features
        - Emotion embeddings
        - Speaking pattern model
        - Create JSON/YAML profile"]

        FineTuner["ðŸ› ï¸ Model Fine-Tuning
        - Adapt Piper TTS model
        - Train user-specific embeddings
        - Save personalized weights"]
    end

    %% OUTPUT
    subgraph "Output"
        VoiceProfile["ðŸ“¦ personalized_voice_profile.json"]
        UpdatedModel["ðŸŽ™ï¸ Personalized Piper Model"]
    end

    %% CONNECTIONS
    Mic --> Preprocessor
    Metadata --> Preprocessor

    Preprocessor --> SpeakingPattern
    Preprocessor --> ProsodyModel
    Preprocessor --> EmotionDetector

    SpeakingPattern --> ProfileBuilder
    ProsodyModel --> ProfileBuilder
    EmotionDetector --> ProfileBuilder

    ProfileBuilder --> FineTuner

    FineTuner --> VoiceProfile
    FineTuner --> UpdatedModel
